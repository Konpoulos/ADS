{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"We’re where we were when W.W.W. Wonka (1940-2012) was when he was elected prime minister of the U.K. with 50.23% of the votes. For more information, see: www.we-want-wonka.co.uk/2012\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tokenize sentence yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We’re', 'where', 'we', 'were', 'when', 'W.W.W.', 'Wonka', '(1940-2012)', 'was', 'when', 'he', 'was', 'elected', 'prime', 'minister', 'of', 'the', 'U.K.', 'with', '50.23%', 'of', 'the', 'votes.', 'For', 'more', 'information,', 'see:', 'www.we-want-wonka.co.uk/2012']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = text.split() # in order to split the text into tokens\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we’re', 'where', 'we', 'were', 'when', 'www', 'wonka', '19402012', 'was', 'when', 'he', 'was', 'elected', 'prime', 'minister', 'of', 'the', 'uk', 'with', '5023', 'of', 'the', 'votes', 'for', 'more', 'information', 'see', 'wwwwewantwonkacouk2012']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    punctuations = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    for punctuation in punctuations:\n",
    "        text = text.replace(punctuation, '')\n",
    "    text = text.lower() \n",
    "    text = text.split()\n",
    "    return text\n",
    "    \n",
    "tokenized_text_mine = tokenize(text)\n",
    "print(tokenized_text_mine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1a. Use the first computational method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 're', 'where', 'we', 'were', 'when', 'w', 'w', 'w', 'wonka', '1940', '2012', 'was', 'when', 'he', 'was', 'elected', 'prime', 'minister', 'of', 'the', 'u', 'k', 'with', '50', '23', 'of', 'the', 'votes', 'for', 'more', 'information', 'see', 'www', 'we', 'want', 'wonka', 'co', 'uk', '2012']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+')#?\n",
    "tokenized_text_nltk = tokenizer.tokenize(text.lower())\n",
    "print(tokenized_text_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1b. Using the second computational method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', '’re', 'where', 'we', 'were', 'when', 'w.w.w', 'wonka', '1940', '2012', 'was', 'when', 'he', 'was', 'elected', 'prime', 'minister', 'of', 'the', 'u.k', 'with', '50.23', 'of', 'the', 'votes', 'for', 'more', 'information', 'see', 'www.we-want-wonka.co.uk/2012']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#nlp = spacy.load('en') the same answer\n",
    "\n",
    "processed_text = nlp(text.lower())\n",
    "\n",
    "tokenized_text_spacy = [token.text for token in processed_text if not token.is_punct]\n",
    "print(tokenized_text_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see 3 different tokenizations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Inspect the lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For mine tokenization we have:  28 words\n"
     ]
    }
   ],
   "source": [
    "print(\" For mine tokenization we have: \" , len(tokenized_text_mine),\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For nltk tokenization we have:  40 words\n"
     ]
    }
   ],
   "source": [
    "print(\" For nltk tokenization we have: \" , len(tokenized_text_nltk),\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For spacy tokenization we have:  30 words\n"
     ]
    }
   ],
   "source": [
    "print(\" For spacy tokenization we have: \" , len(tokenized_text_spacy),\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of them having different answers for me the 'most' correct one is spacy because it keeps everything smooth and i can understand all the words that came out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3 Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will run all 3 tokenizations and see how many occurences those 2 words have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The occurances of we are:  1\n",
      "The occurances of www are:  1\n"
     ]
    }
   ],
   "source": [
    "counts_of_we =0\n",
    "counts_of_www = 0\n",
    "for words in tokenized_text_mine :\n",
    "    if words == 'we':\n",
    "        counts_of_we += 1\n",
    "    elif words == 'www':\n",
    "        counts_of_www += 1\n",
    "        \n",
    "print(\"The occurances of we are: \",counts_of_we)\n",
    "print(\"The occurances of www are: \",counts_of_www)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in mine tokenization they have the same occurancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The occurances of we are:  3\n",
      "The occurances of www are:  1\n"
     ]
    }
   ],
   "source": [
    "counts_of_we =0\n",
    "counts_of_www = 0\n",
    "for words in tokenized_text_nltk :\n",
    "    if words == 'we':\n",
    "        counts_of_we += 1\n",
    "    elif words == 'www':\n",
    "        counts_of_www += 1\n",
    "        \n",
    "print(\"The occurances of we are: \",counts_of_we)\n",
    "print(\"The occurances of www are: \",counts_of_www)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in nltk we has 3 occurancies and www only 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The occurances of we are:  2\n",
      "The occurances of www are:  0\n"
     ]
    }
   ],
   "source": [
    "counts_of_we =0\n",
    "counts_of_www = 0\n",
    "for words in tokenized_text_spacy :\n",
    "    if words == 'we':\n",
    "        counts_of_we += 1\n",
    "    elif words == 'www':\n",
    "        counts_of_www += 1\n",
    "        \n",
    "print(\"The occurances of we are: \",counts_of_we)\n",
    "print(\"The occurances of www are: \",counts_of_www)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in spacy we has 2 occurancies and www 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Compare the ten most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('when', 2),\n",
       " ('was', 2),\n",
       " ('of', 2),\n",
       " ('the', 2),\n",
       " ('we’re', 1),\n",
       " ('where', 1),\n",
       " ('we', 1),\n",
       " ('were', 1),\n",
       " ('www', 1),\n",
       " ('wonka', 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = tokenized_text_mine\n",
    "\n",
    "word_counts = Counter(tokenized_text_mine)\n",
    "word_counts.most_common()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('we', 3),\n",
       " ('w', 3),\n",
       " ('when', 2),\n",
       " ('wonka', 2),\n",
       " ('2012', 2),\n",
       " ('was', 2),\n",
       " ('of', 2),\n",
       " ('the', 2),\n",
       " ('re', 1),\n",
       " ('where', 1)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = tokenized_text_nltk\n",
    "\n",
    "word_counts = Counter(tokenized_text_nltk)\n",
    "word_counts.most_common()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('we', 2),\n",
       " ('when', 2),\n",
       " ('was', 2),\n",
       " ('of', 2),\n",
       " ('the', 2),\n",
       " ('’re', 1),\n",
       " ('where', 1),\n",
       " ('were', 1),\n",
       " ('w.w.w', 1),\n",
       " ('wonka', 1)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = tokenized_text_spacy\n",
    "\n",
    "word_counts = Counter(tokenized_text_spacy)\n",
    "word_counts.most_common()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that all 3 of them have different answers the most frequent for mine is the 'when' for the nltk is 'we' and 'w' and for the space is only 'we' , also the impact of those differences if we have million texts would have a big reflexion in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Better Call Saul</td>\n",
       "      <td>linear</td>\n",
       "      <td>2017</td>\n",
       "      <td>Walter. And there the chain ends.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Better Call Saul</td>\n",
       "      <td>linear</td>\n",
       "      <td>2016</td>\n",
       "      <td>I love this show. But it's hard to argue again...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Better Call Saul</td>\n",
       "      <td>linear</td>\n",
       "      <td>2017</td>\n",
       "      <td>What am I missing?  A lot of reference to ribs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Better Call Saul</td>\n",
       "      <td>linear</td>\n",
       "      <td>2018</td>\n",
       "      <td>Oh come on Mike, he's a good little boy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Better Call Saul</td>\n",
       "      <td>linear</td>\n",
       "      <td>2017</td>\n",
       "      <td>Look again 👀</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              title    type  year  \\\n",
       "0  Better Call Saul  linear  2017   \n",
       "1  Better Call Saul  linear  2016   \n",
       "2  Better Call Saul  linear  2017   \n",
       "3  Better Call Saul  linear  2018   \n",
       "4  Better Call Saul  linear  2017   \n",
       "\n",
       "                                                post  \n",
       "0                  Walter. And there the chain ends.  \n",
       "1  I love this show. But it's hard to argue again...  \n",
       "2  What am I missing?  A lot of reference to ribs...  \n",
       "3          Oh come on Mike, he's a good little boy.   \n",
       "4                                       Look again 👀  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"discussions.p\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title\n",
       "Better Call Saul            5268\n",
       "Black Mirror                4720\n",
       "Breaking Bad                6424\n",
       "Dark                        2004\n",
       "Fargo                        680\n",
       "Game of Thrones            15462\n",
       "House of Cards               158\n",
       "La Casa de Papel             150\n",
       "Mindhunter                   657\n",
       "Mr. Robot                   1347\n",
       "Orange is the New Black     1208\n",
       "Ozark                       1417\n",
       "Stranger Things             2891\n",
       "Succession                   389\n",
       "The Crown                    338\n",
       "The Mandelorian               95\n",
       "The Newsroom                 484\n",
       "The Witcher                 1188\n",
       "True Detective              2721\n",
       "Twin Peaks                  2399\n",
       "Name: post, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = df.groupby('title').count() \n",
    "grouped['post']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 movie has : 15462 Posts\n",
      "1 movie has : 6424 Posts\n",
      "2 movie has : 5268 Posts\n",
      "3 movie has : 4720 Posts\n",
      "4 movie has : 2891 Posts\n",
      "5 movie has : 2721 Posts\n",
      "6 movie has : 2399 Posts\n",
      "7 movie has : 2004 Posts\n",
      "8 movie has : 1417 Posts\n",
      "9 movie has : 1347 Posts\n",
      "10 movie has : 1208 Posts\n",
      "11 movie has : 1188 Posts\n",
      "12 movie has : 680 Posts\n",
      "13 movie has : 657 Posts\n",
      "14 movie has : 484 Posts\n",
      "15 movie has : 389 Posts\n",
      "16 movie has : 338 Posts\n",
      "17 movie has : 158 Posts\n",
      "18 movie has : 150 Posts\n",
      "19 movie has : 95 Posts\n"
     ]
    }
   ],
   "source": [
    "df2 = df.title.value_counts() \n",
    "for index,i in enumerate(df2) :\n",
    "    print (index, \"movie has :\",i , \"Posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>588</td>\n",
       "      <td>588</td>\n",
       "      <td>588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>3342</td>\n",
       "      <td>3342</td>\n",
       "      <td>3342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>2758</td>\n",
       "      <td>2758</td>\n",
       "      <td>2758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>839</td>\n",
       "      <td>839</td>\n",
       "      <td>839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>7118</td>\n",
       "      <td>7118</td>\n",
       "      <td>7118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>7734</td>\n",
       "      <td>7734</td>\n",
       "      <td>7734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>12830</td>\n",
       "      <td>12827</td>\n",
       "      <td>12830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>4332</td>\n",
       "      <td>4332</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>6608</td>\n",
       "      <td>6608</td>\n",
       "      <td>6608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>3851</td>\n",
       "      <td>3851</td>\n",
       "      <td>3851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      title   type   post\n",
       "year                     \n",
       "2011    588    588    588\n",
       "2012   3342   3342   3342\n",
       "2013   2758   2758   2758\n",
       "2014    839    839    839\n",
       "2015   7118   7118   7118\n",
       "2016   7734   7734   7734\n",
       "2017  12830  12827  12830\n",
       "2018   4332   4332   4332\n",
       "2019   6608   6608   6608\n",
       "2020   3851   3851   3851"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = df.groupby('year').count() \n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\tqdm\\std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from pandas import Panel\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca69663ed054cbba8075db6abbfe8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['parsed_posts_no_punctuation'] = df['post'].progress_apply(lambda x: [y.lemma_ for y in nlp(x) if not y.is_punct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce2a2c265ae481c8e7a67f12114d15c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['parsed_posts_nouns'] = df['post'].progress_apply(lambda x: [y.lemma_ for y in nlp(x) if not y.is_punct and y.pos_ == 'NOUN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2047e64c3eb14223b09908e7e1672775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['parsed_posts_adjectives'] = df['post'].progress_apply(lambda x: [y.lemma_ for y in nlp(x) if not y.is_punct and y.pos_ == 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                  [chain]\n",
      "1        [show, pace, slooooowww, writing, acting, dire...\n",
      "2                     [lot, reference, burger, commercial]\n",
      "3                                                    [boy]\n",
      "4                                                       []\n",
      "                               ...                        \n",
      "49995        [top, mushroom, cloud, shape, hell, top, arm]\n",
      "49996                                       [shooter, kid]\n",
      "49997                                              [night]\n",
      "49998                                        [fan, decade]\n",
      "49999                                                   []\n",
      "Name: parsed_posts_nouns, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['parsed_posts_nouns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                       []\n",
      "1        [hard, sooooooooooo, top, boring, different, b...\n",
      "2                                                       []\n",
      "3                                           [good, little]\n",
      "4                                                      [👀]\n",
      "                               ...                        \n",
      "49995                                                [new]\n",
      "49996                                               [mini]\n",
      "49997                                             [absent]\n",
      "49998                                      [mutual, other]\n",
      "49999                                                   []\n",
      "Name: parsed_posts_adjectives, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['parsed_posts_adjectives'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda t: [item for sublist in t for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('good', 2581), ('other', 1611), ('bad', 1529), ('same', 1527), ('more', 1420), ('first', 1275), ('last', 1133), ('whole', 1086), ('great', 1042), ('sure', 1015)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(flatten(df['parsed_posts_adjectives'])).most_common()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('episode', 5243), ('season', 3004), ('time', 2913), ('show', 2700), ('thing', 2625), ('people', 2370), ('scene', 2237), ('way', 2186), ('character', 1865), ('guy', 1597)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(flatten(df['parsed_posts_nouns'])).most_common()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My choice will be Mr.Robot and Mindhunter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35523                               He poisoned his father\n",
      "35524    I really thought that WR was going to kill him...\n",
      "35525                             Shh... let us have this.\n",
      "35526    I’ve got a feeling that Elliott is going to th...\n",
      "35527    I watched a little late but I'm glad to be abl...\n",
      "                               ...                        \n",
      "36865                                               *no u*\n",
      "36866    i'm excited to see what Angela is going to do ...\n",
      "36867    I find it odd that Elliot could 'summon' Mr Ro...\n",
      "36868    And the one with White Rose fucking with Angel...\n",
      "36869    I think if Price actually wanted to live he co...\n",
      "Name: post, Length: 1347, dtype: object\n"
     ]
    }
   ],
   "source": [
    "Mr_Robot_posts = print(df.loc[df['title'] == 'Mr. Robot']['post']) \n",
    "Mr_Robot_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34866                         Even more than lake killing?\n",
      "34867    There was even a subtle cue when the front des...\n",
      "34868              \"This guy does not go to church\"\\n\\nhmm\n",
      "34869    ... especially because they are so turned on b...\n",
      "34870                  And “I’m sorry” when he wet the bed\n",
      "                               ...                        \n",
      "35518    They are unlikely to speak out against conside...\n",
      "35519                     So Holden... Asperger's... ADHD?\n",
      "35520    I jumped more when he tossed those jars (2 of ...\n",
      "35521    Yeah, Brudos is already in prison by this poin...\n",
      "35522    Honestly I find the Brian thing, although shoc...\n",
      "Name: post, Length: 657, dtype: object\n"
     ]
    }
   ],
   "source": [
    "Mindhunter_posts = print(df.loc[df['title'] == 'Mindhunter']['post']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mr_Robot_posts = df.loc[df['title']==\"Mr. Robot\"][\"post\"]\n",
    "Mindhunter_posts = df.loc[df['title']==\"Mindhunter\"][\"post\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf55f54fb7642bbb28d7efe190a05a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1347.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Mr_Robot_adj = Mr_Robot_posts.progress_apply(lambda x: [y.lemma_ for y in nlp(x) if not y.is_punct and y.pos_ == 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e6649f5bbc42699644e3cf9dccadf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=657.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Mindhunter_adj = Mindhunter_posts.progress_apply(lambda x: [y.lemma_ for y in nlp(x) if not y.is_punct and y.pos_ == 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35523                                                   []\n",
      "35524                                          [surprised]\n",
      "35525                                                   []\n",
      "35526                                              [other]\n",
      "35527    [little, late, glad, able, holy, insane, final...\n",
      "                               ...                        \n",
      "36865                                                   []\n",
      "36866              [excited, convinced, nice, same, white]\n",
      "36867                                                [odd]\n",
      "36868                                                   []\n",
      "36869                                       [right, other]\n",
      "Name: post, Length: 1347, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(Mr_Robot_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('good', 70), ('same', 69), ('other', 64), ('next', 43), ('whole', 43), ('more', 43), ('real', 42), ('last', 41), ('bad', 37), ('right', 30), ('first', 30), ('entire', 28), ('great', 28), ('big', 27), ('dead', 27), ('much', 26), ('parallel', 26), ('only', 24), ('sure', 23), ('alternate', 21)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(flatten(Mr_Robot_adj)).most_common()[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('good', 46), ('other', 42), ('more', 32), ('serial', 32), ('own', 28), ('bad', 26), ('real', 25), ('first', 24), ('same', 24), ('little', 24), ('interesting', 21), ('great', 20), ('black', 19), ('much', 19), ('sure', 19), ('many', 19), ('whole', 18), ('last', 17), ('big', 16), ('well', 16)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(flatten(Mindhunter_adj)).most_common()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The adjectives are almost the same to both titles both of them has the 'good' as the most common one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dfa5d7c3bd7490aa203344f3e5894bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1347.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Mr_Robot_nouns = Mr_Robot_posts.progress_apply(lambda x: [y.lemma_ for y in nlp(x) if not y.is_punct and y.pos_ == 'NOUN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69becb1c031f4f359ffbc9578c2963f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=657.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Mindhunter_nouns = Mindhunter_posts.progress_apply(lambda x: [y.lemma_ for y in nlp(x) if not y.is_punct and y.pos_ == 'NOUN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('episode', 234), ('show', 127), ('time', 93), ('people', 92), ('season', 85), ('thing', 78), ('world', 71), ('way', 59), ('end', 52), ('character', 51), ('scene', 51), ('reality', 42), ('life', 41), ('plane', 41), ('one', 40), ('machine', 38), ('theory', 34), ('money', 31), ('shit', 30), ('part', 29)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(flatten(Mr_Robot_nouns)).most_common()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('season', 98), ('show', 76), ('people', 71), ('episode', 69), ('killer', 67), ('thing', 65), ('scene', 59), ('time', 52), ('way', 45), ('character', 42), ('guy', 41), ('life', 38), ('man', 38), ('murder', 38), ('child', 37), ('kid', 35), ('woman', 34), ('lot', 33), ('work', 32), ('point', 30)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(flatten(Mindhunter_nouns)).most_common()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also in nouns there are a lot in common to both titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Compare the most frequent nouns between posts from 2011 and posts from 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e56e9482c24651bb4129e2febdc90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=588.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Nouns_2011 = df[df.year == 2011][\"post\"].progress_apply(lambda x: [y.lemma_ for y in nlp(x) if not y.is_punct and y.pos_ == 'NOUN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9990     The last thing they need is a wildcard.  Jesse...\n",
      "10003                                                 Hey!\n",
      "10004    Yeah, I guess I'm just out of the loop when it...\n",
      "10005    Yeah, Gus' partner even mentioned the 'chirali...\n",
      "10006                                  I think Gus is mad.\n",
      "                               ...                        \n",
      "16365    I felt it was a throwback to last episodes sce...\n",
      "16371             Bryan Cranston is such an amazing actor.\n",
      "16383    Because who else would...\\n\\nOh shit I hadn't ...\n",
      "16389    It seemed more like a love tap to remind Walte...\n",
      "16399                                  I'm sorry, rum ham!\n",
      "Name: post, Length: 588, dtype: object\n"
     ]
    }
   ],
   "source": [
    "posts_2011 = df[df.year == 2011]['post']\n",
    "print(posts_2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6        Maybe when Jimmy gets the $7 million to ABQ, t...\n",
      "13       He was an assistant manager/manager. Definitel...\n",
      "21       I thought that the writers / producers consult...\n",
      "23       Good point. My bad.\\n\\nStill, when hey made it...\n",
      "25                     Didnt Howard help him get that gig?\n",
      "                               ...                        \n",
      "44839    It's not swordsmanship so much as it is dancin...\n",
      "44842    They did show it exploding when Yennefer defle...\n",
      "44846    Eh. It was pretty clear renfri was going to ki...\n",
      "44855                   This is the part where we escape 😅\n",
      "44874                                That’s right I forgot\n",
      "Name: post, Length: 3851, dtype: object\n"
     ]
    }
   ],
   "source": [
    "posts_2020 = df[df.year == 2020]['post']\n",
    "print(posts_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2becc8a1c04a44b3aa4209da76dfe2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3851.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Nouns_2020 = df[df.year == 2020][\"post\"].progress_apply(lambda x: [y.lemma_ for y in nlp(x) if not y.is_punct and y.pos_ == 'NOUN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('episode', 43), ('scene', 35), ('time', 31), ('season', 30), ('thing', 29), ('show', 26), ('car', 21), ('way', 19), ('guy', 18), ('year', 17), ('shit', 15), ('life', 14), ('money', 14), ('end', 13), ('kid', 13), ('fuck', 12), ('part', 12), ('people', 12), ('lot', 12), ('death', 12)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(flatten(Nouns_2011)).most_common()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('episode', 370), ('time', 338), ('season', 326), ('world', 319), ('show', 270), ('thing', 234), ('way', 203), ('scene', 197), ('character', 181), ('people', 156), ('guy', 147), ('point', 133), ('end', 123), ('family', 103), ('life', 99), ('cartel', 94), ('lot', 89), ('one', 88), ('loop', 86), ('man', 83)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(flatten(Nouns_2020)).most_common()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17006802721088435\n",
      "0.025967281225655673\n"
     ]
    }
   ],
   "source": [
    "#we normalize\n",
    "normalization_factor_nouns_2011 = 100/posts_2011.count()\n",
    "normalization_factor_nouns_2020 = 100/posts_2020.count()\n",
    "\n",
    "print(normalization_factor_nouns_2011)\n",
    "print(normalization_factor_nouns_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_of_nouns_2011 = Counter(flatten(Nouns_2011)).most_common()[:20]\n",
    "counts_of_nouns_2020 = Counter(flatten(Nouns_2020)).most_common()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['episode', 7.0], ['scene', 6.0], ['time', 5.0], ['season', 5.0], ['thing', 5.0], ['show', 4.0], ['car', 4.0], ['way', 3.0], ['guy', 3.0], ['year', 3.0], ['shit', 3.0], ['life', 2.0], ['money', 2.0], ['end', 2.0], ['kid', 2.0], ['fuck', 2.0], ['part', 2.0], ['people', 2.0], ['lot', 2.0], ['death', 2.0]]\n"
     ]
    }
   ],
   "source": [
    "count_nouns_2011_list = [list(elem) for elem in counts_of_nouns_2011]\n",
    "for noun in count_nouns_2011_list:\n",
    "    noun[1] = round(noun[1]*normalization_factor_nouns_2011)\n",
    "print(count_nouns_2011_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['episode', 10.0], ['time', 9.0], ['season', 8.0], ['world', 8.0], ['show', 7.0], ['thing', 6.0], ['way', 5.0], ['scene', 5.0], ['character', 5.0], ['people', 4.0], ['guy', 4.0], ['point', 3.0], ['end', 3.0], ['family', 3.0], ['life', 3.0], ['cartel', 2.0], ['lot', 2.0], ['one', 2.0], ['loop', 2.0], ['man', 2.0]]\n"
     ]
    }
   ],
   "source": [
    "count_nouns_2020_list = [list(elem) for elem in counts_of_nouns_2020]\n",
    "for noun in count_nouns_2020_list:\n",
    "    noun[1] = round(noun[1]*normalization_factor_nouns_2020)\n",
    "print(count_nouns_2020_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with 1000 or more would be almos the same ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.6  Track the normalized frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4051f65b864a4fa08018cd365c79fff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=588.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d129d1804d34069b92f1ced5c63a6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3342.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b2af3089ed4615a5661a92c26d8f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2758.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f702e57b41ee4de3859af24b899b76cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=839.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2d833a7b2b4404b52a1657db34fd07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7118.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6cfe2455e04721b926a72df7f328e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7734.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8d7b0fecb242b6a56fc761f76c038c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12830.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09f15f877324b61b18466975b575b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4332.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3002732eb144c8a6b4caf3c584964b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6608.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91ec2dfebab4e53b5e1161edcf4e22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3851.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lemmas_through_the_years = {}\n",
    "years = df['year'].unique()\n",
    "sorted_years = np.sort(years)\n",
    "for year in sorted_years:\n",
    "    lemmas_through_the_years[year] = df.loc[df['year']==year][\"post\"].progress_apply(lambda x: [y.lemma_ for y in nlp(x) if not y.is_punct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "year_lemma_count = defaultdict(list)\n",
    "year_dict = defaultdict(list)\n",
    "index = [] # the years [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
    "data_for_plot = []\n",
    "lemmas_to_track = ['netflix', 'binge', 'original', 'cliffhanger', 'television']\n",
    "current_max_count = 0\n",
    "for year in lemmas_through_the_years:\n",
    "    if df.loc[df['year']==year][\"post\"].count() > current_max_count:\n",
    "        current_max_count = df.loc[df['year']==year][\"post\"].count()\n",
    "for year in lemmas_through_the_years:\n",
    "        index.append(year)\n",
    "        normalizing_factor_for_year = current_max_count/df.loc[df['year']==year][\"post\"].count()\n",
    "        for lemma in lemmas_to_track:\n",
    "            data_for_plot.append(round(Counter(flatten(lemmas_through_the_years[year]))[lemma]*normalizing_factor_for_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seriesObject():\n",
    "    series_object = {}\n",
    "    # A bit of a hack to split the counts for each word\n",
    "    for i in range(0, len(data_for_plot), 10):\n",
    "        chunk = data_for_plot[i:i + 10]\n",
    "        series_object[lemmas_to_track[int(i/10)]] = chunk\n",
    "    return series_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print((index))\n",
    "df = pd.DataFrame(\n",
    "seriesObject(), index=index)\n",
    "ax = df.plot.bar(rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "alphabet_string = string.ascii_uppercase\n",
    "list2 = [\" \"]\n",
    "AB_list = list(alphabet_string) + list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', ' ']\n"
     ]
    }
   ],
   "source": [
    "print (AB_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G\n"
     ]
    }
   ],
   "source": [
    "your_name = 'giannis'\n",
    "import re\n",
    "import random\n",
    "seq2 = \"\"\n",
    "j=0\n",
    "for i in AB_list:\n",
    "    random_letter = str(random.choices(AB_list))\n",
    "    your_name1 = your_name.upper()\n",
    "    if random_letter in your_name1:\n",
    "        seq2 += your_name1[j]\n",
    "        seq2.append(seq2)\n",
    "    else:\n",
    "        seq2 = your_name1[j]\n",
    "print(seq2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S']\n"
     ]
    }
   ],
   "source": [
    "    random_letter = random.choices(AB_list)\n",
    "print(random_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
